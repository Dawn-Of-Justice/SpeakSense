{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  Ive been working on improving my photography s...      1\n",
      "1  Have you seen the new exhibit at the art museu...      1\n",
      "2  I noticed that my email account is running out...      0\n",
      "3  Today I received some news that made my day un...      1\n",
      "4  Ive been meaning to get into hiking more often...      1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ive been working on improving my photography s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Have you seen the new exhibit at the art museu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I noticed that my email account is running out...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today I received some news that made my day un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ive been meaning to get into hiking more often...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>I recently joined a book club and its been a w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>The art of storytelling is a powerful tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>Can you help me set up a reminder for my docto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>I can’t believe it’s already been a year since...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>Ive been reading up on climate change and the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Label\n",
       "0     Ive been working on improving my photography s...      1\n",
       "1     Have you seen the new exhibit at the art museu...      1\n",
       "2     I noticed that my email account is running out...      0\n",
       "3     Today I received some news that made my day un...      1\n",
       "4     Ive been meaning to get into hiking more often...      1\n",
       "...                                                 ...    ...\n",
       "7995  I recently joined a book club and its been a w...      1\n",
       "7996  The art of storytelling is a powerful tool for...      1\n",
       "7997  Can you help me set up a reminder for my docto...      0\n",
       "7998  I can’t believe it’s already been a year since...      1\n",
       "7999  Ive been reading up on climate change and the ...      1\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Genereate a new file with no punctuation\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "def process_folder(folder_path, label):\n",
    "    \"\"\"Reads all text files in a folder, cleans lines, removes punctuation, and returns labeled data.\"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.read()\n",
    "        \n",
    "        # Remove \"SampleX: \" in any case (Sample, sample, SAMPLE, etc.)\n",
    "        cleaned_text = re.sub(r\"(?i)\\bsample\\d+:\\s*\", \"\", lines).strip()\n",
    "        \n",
    "        # Split into lines, remove punctuation, and store non-empty ones with labels\n",
    "        for line in cleaned_text.split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                # Remove all punctuation\n",
    "                line_no_punct = line.translate(str.maketrans('', '', string.punctuation))\n",
    "                cleaned_data.append((line_no_punct.strip(), label))\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Define folder paths\n",
    "addressing_folder = r\"D:\\main project\\Addressing\"\n",
    "non_addressing_folder = r\"D:\\main project\\Non Addressing\"\n",
    "\n",
    "# Process both folders\n",
    "addressing_data = process_folder(addressing_folder, 0)  # Label 0 for Addressing\n",
    "non_addressing_data = process_folder(non_addressing_folder, 1)  # Label 1 for Non-Addressing\n",
    "\n",
    "# Combine and create DataFrame\n",
    "data = addressing_data + non_addressing_data\n",
    "df = pd.DataFrame(data, columns=[\"Text\", \"Label\"])\n",
    "\n",
    "# Shuffle the DataFrame for variety\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df.to_csv(\"cleaned_mixed_data_no_punct.csv\", index=False, encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Code for the Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Monitor validation loss\n",
    "    patience=3,          # Stop after 3 epochs of no improvement\n",
    "    restore_best_weights=True  # Restore the best model weights\n",
    ")\n",
    "\n",
    "# Load data\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Assuming your CSV has columns 'text' and 'label'\n",
    "    # If different, adjust the column names below\n",
    "    texts = df['Text'].values\n",
    "    labels = df['Label'].values\n",
    "    return texts, labels\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_data(texts, labels, max_words=10000, max_sequence_length=1000, validation_split=0.2):\n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        padded_sequences, labels, test_size=validation_split, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, tokenizer\n",
    "\n",
    "# Build RNN model\n",
    "def build_model(vocab_size, embedding_dim=128, max_sequence_length=1000):\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks = [early_stopping]\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Get predictions\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['Addressing Robot (0)', 'Not Addressing Robot (1)']))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Addressing Robot (0)', 'Not Addressing Robot (1)'],\n",
    "                yticklabels=['Addressing Robot (0)', 'Not Addressing Robot (1)'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Configuration\n",
    "    csv_filepath = '/content/cleaned_mixed_data_no_punct.csv'  # Replace with your CSV file path\n",
    "    max_words = 10000\n",
    "    max_sequence_length = 100\n",
    "    embedding_dim = 128\n",
    "    epochs = 4\n",
    "    batch_size = 32\n",
    "    validation_split = 0.2\n",
    "\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    texts, labels = load_data(csv_filepath)\n",
    "    X_train, X_val, y_train, y_val, tokenizer = preprocess_data(\n",
    "        texts, labels, max_words, max_sequence_length, validation_split\n",
    "    )\n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "    model = build_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val, epochs, batch_size)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Plot training history\n",
    "    plot_history(history)\n",
    "\n",
    "    # Save model\n",
    "    model.save('robot_addressing_classifier.h5')\n",
    "    print(\"Model saved as 'robot_addressing_classifier.h5'\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    import pickle\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Tokenizer saved as 'tokenizer.pickle'\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Text: \"Hey robot, what's the weather today?\"\n",
      "Result: This IS addressing the robot\n",
      "Confidence: 1.00\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Text: \"I need to finish my homework soon.\"\n",
      "Result: This is NOT addressing the robot\n",
      "Confidence: 1.00\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Text: \"Robot, can you help me with this?\"\n",
      "Result: This IS addressing the robot\n",
      "Confidence: 1.00\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Text: \"The meeting starts at 2 PM.\"\n",
      "Result: This is NOT addressing the robot\n",
      "Confidence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = tf.keras.models.load_model('robot_addressing_classifier.h5')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "def classify_text(text, max_sequence_length=100):\n",
    "    \"\"\"\n",
    "    Classify a single text input to determine if it's addressing a robot.\n",
    "    \n",
    "    Args:\n",
    "        text: Text string to classify\n",
    "        max_sequence_length: Maximum length for padding (should match training)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Convert to sequence\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded_sequence = pad_sequences(\n",
    "        sequences,\n",
    "        maxlen=max_sequence_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_prob = model.predict(padded_sequence)[0][0]\n",
    "    predicted_class = 1 if prediction_prob > 0.5 else 0\n",
    "    \n",
    "    # Return result\n",
    "    is_addressing_robot = (predicted_class == 0)\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'is_addressing_robot': is_addressing_robot,\n",
    "        'confidence': float(max(prediction_prob, 1 - prediction_prob))\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with different examples\n",
    "    test_examples = [\n",
    "        \"Hey robot, what's the weather today?\",\n",
    "        \"I need to finish my homework soon.\",\n",
    "        \"Robot, can you help me with this?\",\n",
    "        \"The meeting starts at 2 PM.\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_examples:\n",
    "        result = classify_text(text)\n",
    "        status = \"IS\" if result['is_addressing_robot'] else \"is NOT\"\n",
    "        print(f\"Text: \\\"{text}\\\"\")\n",
    "        print(f\"Result: This {status} addressing the robot\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The Eiffel Tower is one of the most iconic landmarks in the world, located in Paris, France. It was designed by Gustave Eiffel and completed in 1889 as the entrance arch for the 1889 Exposition Universelle (World’s Fair).What do you think.',\n",
       " 'is_addressing_robot': True,\n",
       " 'confidence': 0.9999315142631531}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(\"The Eiffel Tower is one of the most iconic landmarks in the world, located in Paris, France. It was designed by Gustave Eiffel and completed in 1889 as the entrance arch for the 1889 Exposition Universelle (World’s Fair).What do you think.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I am an intelligent boy. I am a good student. I am a good citizen. I am a good person. I am a good human being.What do you think',\n",
       " 'is_addressing_robot': True,\n",
       " 'confidence': 0.9999369382858276}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(\"I am an intelligent boy. I am a good student. I am a good citizen. I am a good person. I am a good human being.What do you think\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speaksense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
